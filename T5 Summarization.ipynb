{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqfl2dabScmiIn9cZTQ6nk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_vlf-dXerli","executionInfo":{"status":"ok","timestamp":1683559495648,"user_tz":-180,"elapsed":12687,"user":{"displayName":"Ahmed Toba","userId":"06597376039079993537"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb9680b5-9b1e-47a4-d4de-41a4fc249ae3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}],"source":["!pip install transformers -q\n","!pip install sentencepiece"]},{"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","import seaborn as sns \n","import torch \n","import torch.nn.functional as F \n","from torch.utils.data import Dataset , DataLoader  , RandomSampler ,SequentialSampler \n","\n","from transformers import T5Tokenizer , T5ForConditionalGeneration \n","\n","\n","device =  'cuda' if torch.cuda.is_available() else 'cpu'\n"],"metadata":{"id":"7irSZIjre126","executionInfo":{"status":"ok","timestamp":1683562075476,"user_tz":-180,"elapsed":9,"user":{"displayName":"Ahmed Toba","userId":"06597376039079993537"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAUAg0jxf_5J","executionInfo":{"status":"ok","timestamp":1683558986375,"user_tz":-180,"elapsed":24,"user":{"displayName":"Ahmed Toba","userId":"06597376039079993537"}},"outputId":"7f5d756b-fcec-4632-f608-edace31d2da4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: nvidia-smi: command not found\n"]}]},{"cell_type":"markdown","source":["# class for prepare dataset "],"metadata":{"id":"Rgf7KAFfgdbK"}},{"cell_type":"code","source":["# here we will load data and then tokenize it to return model inputs that is   [source_ids , source_mask ,  target_ids , target_mask]\n","\n","class CustomData(Dataset):\n","  def __init__(self , data  , tokenizer , artical_len ,summ_len):\n","    self.data        = data \n","    self.tokenizer   = tokenizer\n","    self.artical_len = artical_len\n","    self.summ_len    = summ_len \n","    self.text = data.Headline  \n","    self.ctext= data.Short \n","\n","  def __len__(self):\n","    return len(self.text) \n","\n","  def __getitem__(self, index):\n","    ctext = self.ctext[index] \n","    ctext = str(ctext.split())\n","    ctext = ' '.join(ctext)\n","\n","    text  = self.text[index]\n","    text  = str(text.split())\n","    text  = ' '.join(text)\n","\n","\n","    source = self.tokenizer.batch_encode_plus([ctext] , max_length = self.artical_len , pad_to_max_length = True , return_tensor='pt') \n","    target = self.tokenizer.batch_encode_plus([text] , max_length = self.summ_len ,pad_to_max_length = True , return_tensor ='pt')\n","\n","    source_ids = source['input_ids'].squeeze()\n","    source_mask= source['attention_mask'].squeeze()\n","\n","    target_ids = target['input_ids'].squeeze()\n","    target_mask= target['attention_mask'].squeeze() \n","\n","    return {\n","        'source_ids' : source_ids.to(dtype= torch.long),  \n","        'source_mask': source_mask.to(dtype=torch.long) ,\n","        'target_ids' : target_ids.to(dtype=torch.long) ,\n","        'target_mask': target_mask.to(dtype=torch.long)\n","\n","    }"],"metadata":{"id":"fHYUvRDRgIWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## we will creat train function that will be called in main function later \n","\n","def train(epoch , tokenizer , model , loader , optimizer):\n","\n","\n","  model.train()\n","  for _,data in enumerate(loader ,0):\n","    y = data['target_ids'].to(device , dtype=torch.long)\n","    y_ids  = y[: , :-1].contiguous()\n","    lm_labels = y[: , 1:].clone().detach()\n","    lm_labels[ y[: , 1:] == tokenizer.pad_token_id    ] = -100 \n","    ids =  data['source_ids'].to(device ,dtype=torch.long)\n","    mask = data['source_mask'].to(device , dtype=torch.long)\n","\n","    output = model(input_ids = ids ,  attention_mask = mask , decoder_input_ids = y_ids , lm_labels =lm_labels)\n","    loss =output[0]\n","\n","    if _% 500 ==0 :\n","      print(f'epoch: {epoch} , loss: {loss.item()}')\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n"],"metadata":{"id":"MHP1f_aJkQ-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## we will creat evaluate function: this function return predicted value and true value \n","\n","def validation(model , loader , epoch , tokenizer):\n","\n","\n","  model.eval()\n","  predictions =[]\n","  actual =[]\n","  with torch.no_grad(): \n","    for _,data in enumerate(loader ,0):\n","      y   = data['target_ids'].to(device , dtype = torch.long)\n","      ids = data['source_ids'].to(device , dtype = torch.long) \n","      mask= data['source_mask'].to(device , dtype =torch.long)\n","\n","      pred = model.generate(input_ids = ids ,\n","                            attention_mask = mask ,\n","                            max_length =150 , \n","                            num_beams = 2 ,\n","                            repetition_penalty = 2.5 ,\n","                            length_penalty = 1.0 ,\n","                            early_stopping =True\n","                            )\n","      preds  =  [tokenizer.decode(t, skip_special_tokens=True , clean_up_tokenization_spaces =True) for t in pred]\n","      actuals=  [tokenizer.decode(t, skip_special_tokens=True , clean_up_tokenization_spaces =True) for t in y   ]\n","\n","      predictions.extend(preds)\n","      actual.extend(actuals)\n","      if _%100 ==0:\n","        print(f'Completed {_}')\n","  return predictions , actual\n","\n","\n"],"metadata":{"id":"tKCtapKg2f9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## here we will creat our main function to collect all functions and class together to fine-tunnnig our model \n"],"metadata":{"id":"i69m7eXJj707"}},{"cell_type":"code","source":["def main():\n","  ## intiate parameters for train and evaluate \n","  ## preparing data \n","  ## call tokenizer and model \n","  ## fine-tunning our model \n","  ## evaluate our model \n","  ## show our result \n","  parameters={ 'train_batch_size':2 ,\n","              'val_batch_size'   :2 ,\n","\n","              'train_epochs':2,\n","              'val_epochs'  :2,\n","\n","              'learning_rate':1e-4,\n","              'seed':42 ,\n","\n","              'artical_len':512,\n","              'summ_len'   :150}\n","\n","  np.random.seed(parameters['seed'])\n","  torch.manual_seed(parameters['seed'])\n","  torch.backends.cudnn.deterministic =True \n","############################################################\n","############################################################\n","  tokenizer = T5Tokenizer.from_pretrained(\"t5-base\").to(device)\n","  model     = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n","############################################################\n","############################################################\n","  # importing data from googel drive \n","  from google.colab import drive \n","  drive.mount('/content/drive', force_remount=True)\n","  DATA_DIR = \"/content/drive/My Drive/Colab Notebook/Text Summarization With Attention/Inshorts Cleaned Data.xlsx\"\n","  data  =  pd.read_excel( DATA_DIR ,engine = 'openpyxl')\n","  df = data[['Headline' , 'Short'] ]\n","  ## import think to get good performance to prepare data as t5 model trained \n","  # it trained as get type of task first so we will add summarize in beginning of artical text (unsummarized)\n","  df = 'summarize: '+ df.Short \n","  print('samples of our data :\\n')\n","  print(df.head())\n","  ## spliting our data to 80% train data 20% test or validation data \n","  from sklearn.model_selection import train_test_split \n","  train_data, test_data = train_test_split(df , test_size=0.2, shuffle =True , random_state= parameters['seed'])\n","  print(f'shape of all data {df.shape}')\n","  print(f'shape of train data {train_data.shape}')\n","  print(f'shape of test data {test_data.shape }')\n","  ## calling customdata class to get (ids and mask ) for inputs and targets \n","  train_custom = CustomData(train_data , tokenizer , parameters['artical_len'] , parameters['summ_len']) \n","  test_custom  = CustomData(test_data  , tokenizer , parameters['artical_len'] , parameters['summ_len'])\n","  ## now creating data loader \n","  train_loader = DataLoader(train_custom , batch_size = parameters['train_batch_size'] , shuffle=True , num_workers =0) \n","  test_loader  = DataLoader(test_custom  , batch_size = parameters['val_batch_size'] , shuffle =True  , num_workers = 0)\n","############################################################\n","############################################################\n","  ## now we are ready for training our model that's called fine-tunning model \n","  optimizer = torch.optim.Adam(params=model.parameters()  , lr = parameters['learning_rate'])\n","  print(f'Intiating Fine-tunning for the model in our data \\n')\n","  for epoch in parameters['train_epochs'] :\n","    # just calling train function \n","    train(epoch ,tokenizer , model , train_loader ,optimizer)\n","\n","  print(f'Now we started created summray.....\\n')\n","  for epoch in parameters['eval_epochs']:\n","    pred , actual = validation(model , data , epoch ,tokenizer)\n","    new_df = pf.DataFrame({'Generated':pred , \n","                           'Actual':actual})\n","    store_dir = '/content/drive/My Drive/Colab Notebook/Text Summarization With Attention/generated.csv'\n","    new_df.to_csv(store_dir)\n","    print(f'number of generate is {epoch+1}')\n","  \n","\n","\n","\n"],"metadata":{"id":"VWj_DuxAe-Lw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","  main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"id":"uTutUZ5a19qL","executionInfo":{"status":"error","timestamp":1683558986381,"user_tz":-180,"elapsed":21,"user":{"displayName":"Ahmed Toba","userId":"06597376039079993537"}},"outputId":"f45321ea-33b2-4ffb-908f-822734c7eb34"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-174f78df48e2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-51a10b5206e6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mmodel\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"_from_config\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"P5edyT-I2Inz"},"execution_count":null,"outputs":[]}]}